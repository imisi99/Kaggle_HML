{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This is the Building of Models for the kaggle Housing Competition",
   "id": "6daab436c394dcee"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T21:09:16.447779Z",
     "start_time": "2024-11-03T21:09:16.433085Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from utils import *\n",
    "\n",
    "\n",
    "plt.style.use('deeplearning.mplstyle')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "7019e442fd32d2b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T21:09:18.151420Z",
     "start_time": "2024-11-03T21:09:18.092876Z"
    }
   },
   "source": [
    "# Loading, Cleaning and One Hot Encoding Data\n",
    "\n",
    "data = './data/train.csv'\n",
    "df = modifying_data(data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data before processing...\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "Data after preprocessing:\n",
      "   LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  MasVnrArea  \\\n",
      "0         65.0     8450            7            5       2003       196.0   \n",
      "1         80.0     9600            6            8       1976         0.0   \n",
      "2         68.0    11250            7            5       2001       162.0   \n",
      "3         60.0     9550            7            5       1915         0.0   \n",
      "4         84.0    14260            8            5       2000       350.0   \n",
      "\n",
      "   BsmtFinSF1  BsmtUnfSF  TotalBsmtSF  1stFlrSF  ...  BsmtHalfBath  FullBath  \\\n",
      "0         706        150          856       856  ...             0         2   \n",
      "1         978        284         1262      1262  ...             1         2   \n",
      "2         486        434          920       920  ...             0         2   \n",
      "3         216        540          756       961  ...             0         1   \n",
      "4         655        490         1145      1145  ...             0         2   \n",
      "\n",
      "   HalfBath  BedroomAbvGr  KitchenAbvGr  TotRmsAbvGrd  Fireplaces  GarageCars  \\\n",
      "0         1             3             1             8           0           2   \n",
      "1         0             3             1             6           1           2   \n",
      "2         1             3             1             6           1           2   \n",
      "3         0             3             1             7           1           3   \n",
      "4         1             4             1             9           1           3   \n",
      "\n",
      "   PoolArea  SalePrice  \n",
      "0         0     208500  \n",
      "1         0     181500  \n",
      "2         0     223500  \n",
      "3         0     140000  \n",
      "4         0     250000  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "35f9a22585293fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T21:09:02.175633Z",
     "start_time": "2024-11-03T21:09:02.157723Z"
    }
   },
   "source": [
    "# Splitting Data into train and cross validation set\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "Y = df['SalePrice']\n",
    "print(len(X))\n",
    "\n",
    "X = X.values\n",
    "Y = Y.values\n",
    "\n",
    "X_train, X_cv, Y_train, Y_cv = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_cv shape: {X_cv.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "print(f'Y_cv shape: {Y_cv.shape}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "X_train shape: (1168, 22)\n",
      "X_cv shape: (292, 22)\n",
      "Y_train shape: (1168,)\n",
      "Y_cv shape: (292,)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "dae420322447f6d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T21:09:02.734290Z",
     "start_time": "2024-11-03T21:09:02.722678Z"
    }
   },
   "source": [
    "# viewing the split data set\n",
    "\n",
    "print(f'The first two training input: {X_train[:2]}')\n",
    "print(f'The first two cross validation input: {X_cv[:2]}')\n",
    "print(f'The first five training target: {Y_train[:5]}')\n",
    "print(f'The first five cross validation target: {Y_cv[:5]}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two training input: [[7.000e+01 8.400e+03 5.000e+00 6.000e+00 1.957e+03 0.000e+00 9.220e+02\n",
      "  3.920e+02 1.314e+03 1.314e+03 0.000e+00 1.314e+03 1.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 3.000e+00 1.000e+00 5.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00]\n",
      " [5.900e+01 7.837e+03 6.000e+00 7.000e+00 1.993e+03 0.000e+00 0.000e+00\n",
      "  7.990e+02 7.990e+02 7.990e+02 7.720e+02 1.571e+03 0.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 1.000e+00 7.000e+00 1.000e+00 2.000e+00\n",
      "  0.000e+00]]\n",
      "The first two cross validation input: [[7.0000e+01 8.4140e+03 6.0000e+00 8.0000e+00 1.9630e+03 0.0000e+00\n",
      "  6.6300e+02 3.9600e+02 1.0590e+03 1.0680e+03 0.0000e+00 1.0680e+03\n",
      "  0.0000e+00 1.0000e+00 1.0000e+00 0.0000e+00 3.0000e+00 1.0000e+00\n",
      "  6.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00]\n",
      " [9.8000e+01 1.2256e+04 8.0000e+00 5.0000e+00 1.9940e+03 3.6200e+02\n",
      "  1.0320e+03 4.3100e+02 1.4630e+03 1.5000e+03 1.1220e+03 2.6220e+03\n",
      "  1.0000e+00 0.0000e+00 2.0000e+00 1.0000e+00 3.0000e+00 1.0000e+00\n",
      "  9.0000e+00 2.0000e+00 2.0000e+00 0.0000e+00]]\n",
      "The first five training target: [145000 178000  85000 175000 127000]\n",
      "The first five cross validation target: [154500 325000 115000 159000 315500]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Linear Regression Model ",
   "id": "efebb84d0b4cdb6b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "659c1b713e01c8b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:54:03.363659Z",
     "start_time": "2024-10-31T17:53:55.952194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with 1 degree polynomial\n",
      "Predictions for degree 1: [137058.76828771 189319.66027516  67390.59292446 168498.92057763\n",
      " 131949.50296506]\n",
      "Training Model with 2 degree polynomial\n",
      "Predictions for degree 2: [143587.47562301 174042.36856614  84896.66430489 156817.40965018\n",
      " 136845.96029415]\n",
      "Training Model with 3 degree polynomial\n",
      "Predictions for degree 3: [144999.99999996 178000.00000001  84999.99999999 174999.99999995\n",
      " 126999.99999996]\n",
      "Training Model with 4 degree polynomial\n",
      "Predictions for degree 4: [145000.00147341 178000.02891634  85000.0045719  175000.00981043\n",
      " 127000.03766249]\n",
      "Target: [145000 178000  85000 175000 127000]\n",
      "Degree: 1, Train MSE: 611775199.4347619, Train RMSE: 24734.08982426404\n",
      "Degree: 1, CV MSE: 674883530.6252548, CV RMSE: 25978.520562673595\n",
      "Degree: 2, Train MSE: 175541266.12548795, Train RMSE: 13249.198697486876\n",
      "Degree: 2, CV MSE: 728284465.36983, CV RMSE: 26986.74610563174\n",
      "Degree: 3, Train MSE: 24782.770547945205, Train RMSE: 157.42544441082327\n",
      "Degree: 3, CV MSE: 7569708315060.882, CV RMSE: 2751310.2905817223\n",
      "Degree: 4, Train MSE: 24782.770737169758, Train RMSE: 157.4254450118206\n",
      "Degree: 4, CV MSE: 2.2919126214346287e+21, CV RMSE: 47873924232.66165\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building a linear regression model\n",
    "train_mse = []\n",
    "cv_mse = []\n",
    "polys = []\n",
    "models = []\n",
    "scalers = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    poly = PolynomialFeatures(degree=i, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_cv_poly = poly.transform(X_cv)\n",
    "    polys.append(poly)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "    X_cv_scaled = scaler.transform(X_cv_poly)\n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    print(f'Training Model with {i} degree polynomial')\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, Y_train)\n",
    "    models.append(model)\n",
    "    \n",
    "    yhat = model.predict(X_train_scaled)\n",
    "    error = mean_squared_error(Y_train, yhat) / 2\n",
    "    train_mse.append(error)\n",
    "    \n",
    "    yhat_cv = model.predict(X_cv_scaled)\n",
    "    error_cv = mean_squared_error(Y_cv, yhat_cv) / 2\n",
    "    cv_mse.append(error_cv)\n",
    "    print(f'Predictions for degree {i}: {yhat[:5]}')\n",
    "# Printing the prediction and the target value\n",
    "print(f'Target: {Y_train[:5]}')\n",
    "# printing results\n",
    "for i in range(len(train_mse)):\n",
    "    print(f'Degree: {i + 1}, Train MSE: {train_mse[i]}, Train RMSE: {np.sqrt(train_mse[i])}')\n",
    "    print(f'Degree: {i + 1}, CV MSE: {cv_mse[i]}, CV RMSE: {np.sqrt(cv_mse[i])}')\n",
    "    \n",
    "# Plotting the training and cross validation error\n",
    "plt.plot(train_mse, label='Training Error')\n",
    "plt.plot(cv_mse, label='Cross Validation Error')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d72ecbef253cdab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:54:12.082662Z",
     "start_time": "2024-10-31T17:54:12.067284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is with degree : 1\n"
     ]
    }
   ],
   "source": [
    "# Selecting the best model based on the cross validation set\n",
    "degree = np.argmin(cv_mse) + 1\n",
    "print(f'Best Model is with degree : {degree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028310f4-70e8-49a2-86c1-6f3b1c3b535b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:54:14.743713Z",
     "start_time": "2024-10-31T17:54:14.734363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14949\n"
     ]
    }
   ],
   "source": [
    "# checking to see the scaler input \n",
    "new_scaler = scalers[degree - 1]\n",
    "print(scaler.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156ef2cba324b316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:54:25.627358Z",
     "start_time": "2024-10-31T17:54:25.565994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data before processing...\n",
      "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
      "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
      "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
      "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
      "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
      "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
      "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
      "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
      "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
      "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       0      6    2010        WD         Normal  \n",
      "1   12500      6    2010        WD         Normal  \n",
      "2       0      3    2010        WD         Normal  \n",
      "3       0      6    2010        WD         Normal  \n",
      "4       0      1    2010        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "Data after preprocessing:\n",
      "   LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  MasVnrArea  \\\n",
      "0         80.0    11622            5            6       1961         0.0   \n",
      "1         81.0    14267            6            6       1958       108.0   \n",
      "2         74.0    13830            5            5       1997         0.0   \n",
      "3         78.0     9978            6            6       1998        20.0   \n",
      "4         43.0     5005            8            5       1992         0.0   \n",
      "\n",
      "   BsmtFinSF1  BsmtUnfSF  TotalBsmtSF  1stFlrSF  ...  BsmtFullBath  \\\n",
      "0       468.0      270.0        882.0       896  ...           0.0   \n",
      "1       923.0      406.0       1329.0      1329  ...           0.0   \n",
      "2       791.0      137.0        928.0       928  ...           0.0   \n",
      "3       602.0      324.0        926.0       926  ...           0.0   \n",
      "4       263.0     1017.0       1280.0      1280  ...           0.0   \n",
      "\n",
      "   BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  KitchenAbvGr  TotRmsAbvGrd  \\\n",
      "0           0.0         1         0             2             1             5   \n",
      "1           0.0         1         1             3             1             6   \n",
      "2           0.0         2         1             3             1             6   \n",
      "3           0.0         2         1             3             1             7   \n",
      "4           0.0         2         0             2             1             5   \n",
      "\n",
      "   Fireplaces  GarageCars  PoolArea  \n",
      "0           0         1.0         0  \n",
      "1           0         1.0         0  \n",
      "2           1         2.0         0  \n",
      "3           1         2.0         0  \n",
      "4           0         2.0         0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Test input after scaling: [[-0.22681341 -0.20824661  0.64257719 -0.52417405  1.04474171  0.53720882\n",
      "   0.56655159 -0.94267163 -0.46714455 -0.81072855  1.14171318  0.34833198\n",
      "   1.10531958 -0.24287002  0.7736639   1.23694711  0.13621832 -0.21275711\n",
      "   0.88841597 -0.95859215  0.29509165 -0.07099284]\n",
      " [ 0.44104726 -0.1013172  -0.08893368  2.16500001  0.16418327 -0.5978887\n",
      "   1.15930636 -0.64233999  0.45456121  0.23969786 -0.80192292 -0.50629039\n",
      "  -0.81869424  3.99104322  0.7736639  -0.76409752  0.13621832 -0.21275711\n",
      "  -0.34690528  0.59214972  0.29509165 -0.07099284]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the test data\n",
    "test_data = './data/test.csv'\n",
    "df_test = modifying_data(test_data)\n",
    "\n",
    "test_data = df_test.values\n",
    "test_data = polys[degree - 1].transform(test_data)\n",
    "scaler = scalers[degree - 1]\n",
    "X_test_scaled = scaler.transform(X)\n",
    "print(f'Test input after scaling: {X_test_scaled[:2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907083f80da54ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:54:37.994748Z",
     "start_time": "2024-10-31T17:54:37.985344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228165.26016326 185219.06353252 222441.39301355 197084.34974034\n",
      " 288848.70471463 167358.57587426 264046.84182373 247564.63514617\n",
      " 177391.38480716 107316.65009545]\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "# Using the best model to predict the test data\n",
    "yhat_test = models[degree - 1].predict(X_test_scaled)\n",
    "print(yhat_test[:10])\n",
    "print(len(yhat_test))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Building a Neural Network model",
   "id": "61730e7bbd23698a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e2ddb7b86c39715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:55:30.783115Z",
     "start_time": "2024-10-31T17:55:30.771843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using tensorflow to build a model\n",
    "\n",
    "def build_nn_model():\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    model1 = Sequential(\n",
    "        [\n",
    "            Dense(120, activation='relu'),\n",
    "            Dense(60, activation='relu'),\n",
    "            Dense(30, activation='relu'),\n",
    "            Dense(1, activation='relu')\n",
    "        ],\n",
    "        name='model1'\n",
    "    )\n",
    "\n",
    "    model2 = Sequential(\n",
    "        [\n",
    "            Dense(200, activation='relu'),\n",
    "            Dense(150, activation='relu'),\n",
    "            Dense(120, activation='relu'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(75, activation='relu'),\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1, activation='relu')\n",
    "        ],\n",
    "        name='model2'\n",
    "    )\n",
    "\n",
    "    model3 = Sequential(\n",
    "        [\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1, activation='relu')\n",
    "        ],\n",
    "        name='model3'\n",
    "    )\n",
    "\n",
    "    model4 = Sequential(\n",
    "        [\n",
    "            Dense(300, activation='relu'),\n",
    "            Dense(250, activation='relu'),\n",
    "            Dense(200, activation='relu'),\n",
    "            Dense(150, activation='relu'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1, activation='relu')\n",
    "        ],\n",
    "        name='model4'\n",
    "    )\n",
    "\n",
    "    models_nn = [model1, model2, model3, model4]\n",
    "    return models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be991a167152115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:55:39.640008Z",
     "start_time": "2024-10-31T17:55:39.597044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled: [[-0.00419319 -0.21289571 -0.82044456  0.3722173  -0.45546896 -0.5978887\n",
      "   1.03726861 -0.40028165  0.57261219  0.37423523 -0.80192292 -0.40709315\n",
      "   1.10531958 -0.24287002 -1.05556573 -0.76409752  0.13621832 -0.21275711\n",
      "  -0.96456591 -0.95859215 -1.05654384 -0.07099284]\n",
      " [-0.49395768 -0.26524463 -0.08893368  1.26860866  0.71860895 -0.5978887\n",
      "  -0.97199573  0.51191969 -0.59654659 -0.95820221  0.9550877   0.08317013\n",
      "  -0.81869424 -0.24287002  0.7736639   1.23694711  0.13621832 -0.21275711\n",
      "   0.27075534  0.59214972  0.29509165 -0.07099284]]\n",
      "X_cv_scaled: [[-0.00419319 -0.21159396 -0.08893368  2.16500001 -0.25978931 -0.5978887\n",
      "   0.47284403 -0.39131652 -0.00629167 -0.26223003 -0.80192292 -0.87637239\n",
      "  -0.81869424  3.99104322 -1.05556573 -0.76409752  0.13621832 -0.21275711\n",
      "  -0.34690528 -0.95859215 -1.05654384 -0.07099284]\n",
      " [ 1.24248005  0.14564323  1.37408806 -0.52417405  0.75122223  1.49856692\n",
      "   1.27698562 -0.31287169  0.91087367  0.85546506  1.75165987  2.08809894\n",
      "   1.10531958 -0.24287002  0.7736639   1.23694711  0.13621832 -0.21275711\n",
      "   1.50607659  2.14289159  0.29509165 -0.07099284]]\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data for the neural network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cv_scaled = scaler.transform(X_cv)\n",
    "print(f'X_train_scaled: {X_train_scaled[:2]}')\n",
    "print(f'X_cv_scaled: {X_cv_scaled[:2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c4d0a8e422688a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:58:58.326510Z",
     "start_time": "2024-10-31T17:55:40.889997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: model1\n",
      "Epoch 1/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 4ms/step - loss: 41.8688 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2.4556 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1.2465 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.7796 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.5336 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4658 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.3996 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.3420 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.3098 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2427 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2250 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.1903 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1687 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1708 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.2937 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4274 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2941 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2402 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.2499 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1537 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.1826 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1908 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1615 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1435 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0781 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1009 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1056 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0778 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0812 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0660 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0633 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0767 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 17ms/step - loss: 0.0670 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 35ms/step - loss: 0.0688 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 32ms/step - loss: 0.0542 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 16ms/step - loss: 0.0481 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 16ms/step - loss: 0.0503 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 24ms/step - loss: 0.0758 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - loss: 0.0868 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0837 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2193 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.3020 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2087 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.3438 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1.0378 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2991 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1833 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2578 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2707 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2107 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001B[1m30/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0931\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0952 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0671 - learning_rate: 1.0000e-03\n",
      "Epoch 53/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0270 - learning_rate: 1.0000e-03\n",
      "Epoch 54/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0245 - learning_rate: 1.0000e-03\n",
      "Epoch 55/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0224 - learning_rate: 1.0000e-03\n",
      "Epoch 56/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0211 - learning_rate: 1.0000e-03\n",
      "Epoch 57/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0202 - learning_rate: 1.0000e-03\n",
      "Epoch 58/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0194 - learning_rate: 1.0000e-03\n",
      "Epoch 59/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0188 - learning_rate: 1.0000e-03\n",
      "Epoch 60/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0180 - learning_rate: 1.0000e-03\n",
      "Epoch 61/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 62/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 63/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 64/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 65/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 66/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0161 - learning_rate: 1.0000e-03\n",
      "Epoch 67/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0158 - learning_rate: 1.0000e-03\n",
      "Epoch 68/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0156 - learning_rate: 1.0000e-03\n",
      "Epoch 69/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0154 - learning_rate: 1.0000e-03\n",
      "Epoch 70/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0151 - learning_rate: 1.0000e-03\n",
      "Epoch 71/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0150 - learning_rate: 1.0000e-03\n",
      "Epoch 72/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0147 - learning_rate: 1.0000e-03\n",
      "Epoch 73/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0145 - learning_rate: 1.0000e-03\n",
      "Epoch 74/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0143 - learning_rate: 1.0000e-03\n",
      "Epoch 75/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0141 - learning_rate: 1.0000e-03\n",
      "Epoch 76/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0139 - learning_rate: 1.0000e-03\n",
      "Epoch 77/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0137 - learning_rate: 1.0000e-03\n",
      "Epoch 78/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0136 - learning_rate: 1.0000e-03\n",
      "Epoch 79/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0132 - learning_rate: 1.0000e-03\n",
      "Epoch 80/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0132 - learning_rate: 1.0000e-03\n",
      "Epoch 81/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0131 - learning_rate: 1.0000e-03\n",
      "Epoch 82/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0129 - learning_rate: 1.0000e-03\n",
      "Epoch 83/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0127 - learning_rate: 1.0000e-03\n",
      "Epoch 84/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0126 - learning_rate: 1.0000e-03\n",
      "Epoch 85/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0125 - learning_rate: 1.0000e-03\n",
      "Epoch 86/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0123 - learning_rate: 1.0000e-03\n",
      "Epoch 87/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0122 - learning_rate: 1.0000e-03\n",
      "Epoch 88/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0121 - learning_rate: 1.0000e-03\n",
      "Epoch 89/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0120 - learning_rate: 1.0000e-03\n",
      "Epoch 90/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0118 - learning_rate: 1.0000e-03\n",
      "Epoch 91/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0117 - learning_rate: 1.0000e-03\n",
      "Epoch 92/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0116 - learning_rate: 1.0000e-03\n",
      "Epoch 93/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0115 - learning_rate: 1.0000e-03\n",
      "Epoch 94/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0114 - learning_rate: 1.0000e-03\n",
      "Epoch 95/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0113 - learning_rate: 1.0000e-03\n",
      "Epoch 96/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0112 - learning_rate: 1.0000e-03\n",
      "Epoch 97/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0111 - learning_rate: 1.0000e-03\n",
      "Epoch 98/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 99/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 100/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 101/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 102/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0107 - learning_rate: 1.0000e-03\n",
      "Epoch 103/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0106 - learning_rate: 1.0000e-03\n",
      "Epoch 104/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0105 - learning_rate: 1.0000e-03\n",
      "Epoch 105/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0104 - learning_rate: 1.0000e-03\n",
      "Epoch 106/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0103 - learning_rate: 1.0000e-03\n",
      "Epoch 107/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0102 - learning_rate: 1.0000e-03\n",
      "Epoch 108/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0102 - learning_rate: 1.0000e-03\n",
      "Epoch 109/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0101 - learning_rate: 1.0000e-03\n",
      "Epoch 110/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0100 - learning_rate: 1.0000e-03\n",
      "Epoch 111/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0099 - learning_rate: 1.0000e-03\n",
      "Epoch 112/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0099 - learning_rate: 1.0000e-03\n",
      "Epoch 113/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0098 - learning_rate: 1.0000e-03\n",
      "Epoch 114/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0097 - learning_rate: 1.0000e-03\n",
      "Epoch 115/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0096 - learning_rate: 1.0000e-03\n",
      "Epoch 116/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0096 - learning_rate: 1.0000e-03\n",
      "Epoch 117/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0094 - learning_rate: 1.0000e-03\n",
      "Epoch 118/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0094 - learning_rate: 1.0000e-03\n",
      "Epoch 119/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0094 - learning_rate: 1.0000e-03\n",
      "Epoch 120/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0093 - learning_rate: 1.0000e-03\n",
      "Epoch 121/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0092 - learning_rate: 1.0000e-03\n",
      "Epoch 122/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0091 - learning_rate: 1.0000e-03\n",
      "Epoch 123/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0091 - learning_rate: 1.0000e-03\n",
      "Epoch 124/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0090 - learning_rate: 1.0000e-03\n",
      "Epoch 125/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0089 - learning_rate: 1.0000e-03\n",
      "Epoch 126/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0089 - learning_rate: 1.0000e-03\n",
      "Epoch 127/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0088 - learning_rate: 1.0000e-03\n",
      "Epoch 128/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0087 - learning_rate: 1.0000e-03\n",
      "Epoch 129/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0087 - learning_rate: 1.0000e-03\n",
      "Epoch 130/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0086 - learning_rate: 1.0000e-03\n",
      "Epoch 131/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0086 - learning_rate: 1.0000e-03\n",
      "Epoch 132/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0085 - learning_rate: 1.0000e-03\n",
      "Epoch 133/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0084 - learning_rate: 1.0000e-03\n",
      "Epoch 134/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0084 - learning_rate: 1.0000e-03\n",
      "Epoch 135/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0084 - learning_rate: 1.0000e-03\n",
      "Epoch 136/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0082 - learning_rate: 1.0000e-03\n",
      "Epoch 137/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0082 - learning_rate: 1.0000e-03\n",
      "Epoch 138/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0081 - learning_rate: 1.0000e-03\n",
      "Epoch 139/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0081 - learning_rate: 1.0000e-03\n",
      "Epoch 140/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0080 - learning_rate: 1.0000e-03\n",
      "Epoch 141/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0079 - learning_rate: 1.0000e-03\n",
      "Epoch 142/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-03\n",
      "Epoch 143/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0078 - learning_rate: 1.0000e-03\n",
      "Epoch 144/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-03\n",
      "Epoch 145/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0077 - learning_rate: 1.0000e-03\n",
      "Epoch 146/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-03\n",
      "Epoch 147/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0075 - learning_rate: 1.0000e-03\n",
      "Epoch 148/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-03\n",
      "Epoch 149/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0074 - learning_rate: 1.0000e-03\n",
      "Epoch 150/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-03\n",
      "Epoch 151/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0073 - learning_rate: 1.0000e-03\n",
      "Epoch 152/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0072 - learning_rate: 1.0000e-03\n",
      "Epoch 153/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0072 - learning_rate: 1.0000e-03\n",
      "Epoch 154/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0071 - learning_rate: 1.0000e-03\n",
      "Epoch 155/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0070 - learning_rate: 1.0000e-03\n",
      "Epoch 156/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0070 - learning_rate: 1.0000e-03\n",
      "Epoch 157/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0069 - learning_rate: 1.0000e-03\n",
      "Epoch 158/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0069 - learning_rate: 1.0000e-03\n",
      "Epoch 159/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0068 - learning_rate: 1.0000e-03\n",
      "Epoch 160/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0068 - learning_rate: 1.0000e-03\n",
      "Epoch 161/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0067 - learning_rate: 1.0000e-03\n",
      "Epoch 162/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0066 - learning_rate: 1.0000e-03\n",
      "Epoch 163/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0066 - learning_rate: 1.0000e-03\n",
      "Epoch 164/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0065 - learning_rate: 1.0000e-03\n",
      "Epoch 165/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0064 - learning_rate: 1.0000e-03\n",
      "Epoch 166/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0064 - learning_rate: 1.0000e-03\n",
      "Epoch 167/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0063 - learning_rate: 1.0000e-03\n",
      "Epoch 168/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0063 - learning_rate: 1.0000e-03\n",
      "Epoch 169/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0062 - learning_rate: 1.0000e-03\n",
      "Epoch 170/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0062 - learning_rate: 1.0000e-03\n",
      "Epoch 171/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0061 - learning_rate: 1.0000e-03\n",
      "Epoch 172/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0061 - learning_rate: 1.0000e-03\n",
      "Epoch 173/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0061 - learning_rate: 1.0000e-03\n",
      "Epoch 174/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0060 - learning_rate: 1.0000e-03\n",
      "Epoch 175/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0060 - learning_rate: 1.0000e-03\n",
      "Epoch 176/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0059 - learning_rate: 1.0000e-03\n",
      "Epoch 177/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0059 - learning_rate: 1.0000e-03\n",
      "Epoch 178/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 179/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 180/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 181/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0057 - learning_rate: 1.0000e-03\n",
      "Epoch 182/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0057 - learning_rate: 1.0000e-03\n",
      "Epoch 183/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0056 - learning_rate: 1.0000e-03\n",
      "Epoch 184/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 185/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0056 - learning_rate: 1.0000e-03\n",
      "Epoch 186/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0057 - learning_rate: 1.0000e-03\n",
      "Epoch 187/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0057 - learning_rate: 1.0000e-03\n",
      "Epoch 188/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0061 - learning_rate: 1.0000e-03\n",
      "Epoch 189/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0067 - learning_rate: 1.0000e-03\n",
      "Epoch 190/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0061 - learning_rate: 1.0000e-03\n",
      "Epoch 191/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-03\n",
      "Epoch 192/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0099 - learning_rate: 1.0000e-03\n",
      "Epoch 193/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0127 - learning_rate: 1.0000e-03\n",
      "Epoch 194/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0196 - learning_rate: 1.0000e-03\n",
      "Epoch 195/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0220 - learning_rate: 1.0000e-03\n",
      "Epoch 196/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0207 - learning_rate: 1.0000e-03\n",
      "Epoch 197/300\n",
      "\u001B[1m22/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0159\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0158 - learning_rate: 1.0000e-03\n",
      "Epoch 198/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0150 - learning_rate: 1.0000e-04\n",
      "Epoch 199/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0089 - learning_rate: 1.0000e-04\n",
      "Epoch 200/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 201/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0073 - learning_rate: 1.0000e-04\n",
      "Epoch 202/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0069 - learning_rate: 1.0000e-04\n",
      "Epoch 203/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0066 - learning_rate: 1.0000e-04\n",
      "Epoch 204/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0064 - learning_rate: 1.0000e-04\n",
      "Epoch 205/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0063 - learning_rate: 1.0000e-04\n",
      "Epoch 206/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0061 - learning_rate: 1.0000e-04\n",
      "Epoch 207/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0060 - learning_rate: 1.0000e-04\n",
      "Epoch 208/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0059 - learning_rate: 1.0000e-04\n",
      "Epoch 209/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0058 - learning_rate: 1.0000e-04\n",
      "Epoch 210/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0058 - learning_rate: 1.0000e-04\n",
      "Epoch 211/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0057 - learning_rate: 1.0000e-04\n",
      "Epoch 212/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0056 - learning_rate: 1.0000e-04\n",
      "Epoch 213/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0056 - learning_rate: 1.0000e-04\n",
      "Epoch 214/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0055 - learning_rate: 1.0000e-04\n",
      "Epoch 215/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0055 - learning_rate: 1.0000e-04\n",
      "Epoch 216/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0054 - learning_rate: 1.0000e-04\n",
      "Epoch 217/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0054 - learning_rate: 1.0000e-04\n",
      "Epoch 218/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0053 - learning_rate: 1.0000e-04\n",
      "Epoch 219/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0053 - learning_rate: 1.0000e-04\n",
      "Epoch 220/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0053 - learning_rate: 1.0000e-04\n",
      "Epoch 221/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0052 - learning_rate: 1.0000e-04\n",
      "Epoch 222/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0052 - learning_rate: 1.0000e-04\n",
      "Epoch 223/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0052 - learning_rate: 1.0000e-04\n",
      "Epoch 224/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0051 - learning_rate: 1.0000e-04\n",
      "Epoch 225/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0051 - learning_rate: 1.0000e-04\n",
      "Epoch 226/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0051 - learning_rate: 1.0000e-04\n",
      "Epoch 227/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0050 - learning_rate: 1.0000e-04\n",
      "Epoch 228/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0050 - learning_rate: 1.0000e-04\n",
      "Epoch 229/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0050 - learning_rate: 1.0000e-04\n",
      "Epoch 230/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0050 - learning_rate: 1.0000e-04\n",
      "Epoch 231/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 232/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 233/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 234/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 235/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 236/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 237/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 238/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 239/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 240/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 241/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0047 - learning_rate: 1.0000e-04\n",
      "Epoch 242/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0047 - learning_rate: 1.0000e-04\n",
      "Epoch 243/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0047 - learning_rate: 1.0000e-04\n",
      "Epoch 244/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0047 - learning_rate: 1.0000e-04\n",
      "Epoch 245/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0047 - learning_rate: 1.0000e-04\n",
      "Epoch 246/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 247/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 248/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 249/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 250/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 251/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0046 - learning_rate: 1.0000e-04\n",
      "Epoch 252/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 253/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 254/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 255/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 256/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 257/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 258/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0045 - learning_rate: 1.0000e-04\n",
      "Epoch 259/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 260/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 261/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 262/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 263/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 264/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0043 - learning_rate: 1.0000e-04\n",
      "Epoch 265/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0043 - learning_rate: 1.0000e-04\n",
      "Epoch 266/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0043 - learning_rate: 1.0000e-04\n",
      "Epoch 267/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0043 - learning_rate: 1.0000e-04\n",
      "Epoch 268/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0043 - learning_rate: 1.0000e-04\n",
      "Epoch 269/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0043 - learning_rate: 1.0000e-04\n",
      "Epoch 270/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 271/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 272/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 273/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 274/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 275/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 276/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 277/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 278/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 279/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 280/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 281/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 282/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 283/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0041 - learning_rate: 1.0000e-04\n",
      "Epoch 284/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 285/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 286/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 287/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 288/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 289/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 290/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0040 - learning_rate: 1.0000e-04\n",
      "Epoch 291/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 292/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 293/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 294/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 295/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 296/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 297/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 298/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 299/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 300/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0038 - learning_rate: 1.0000e-04\n",
      "Done Training: model1\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step \n",
      "[[136030.3 ]\n",
      " [185388.52]\n",
      " [ 93108.14]\n",
      " [165582.25]\n",
      " [128618.82]]\n",
      "Training Model: model2\n",
      "Epoch 1/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 6ms/step - loss: 39.7697 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 2.1854 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.8967 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.4692 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.3206 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1945 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.1200 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1083 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0658 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0662 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0565 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0477 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0395 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0371 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0395 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0568 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0712 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1197 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.1654 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.1021 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0920 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0780 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0434 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0211 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0144 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0121 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0106 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0101 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0095 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0096 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0084 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0079 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0072 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0073 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0074 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0072 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0087 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0098 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0156 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0232 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0267 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0420 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0676 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0473 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0405 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0147 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0094 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0095 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0124 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001B[1m20/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0148 \n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0140 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0106 - learning_rate: 1.0000e-03\n",
      "Epoch 56/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0060 - learning_rate: 1.0000e-03\n",
      "Epoch 57/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0055 - learning_rate: 1.0000e-03\n",
      "Epoch 58/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0053 - learning_rate: 1.0000e-03\n",
      "Epoch 59/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0052 - learning_rate: 1.0000e-03\n",
      "Epoch 60/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0051 - learning_rate: 1.0000e-03\n",
      "Epoch 61/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0049 - learning_rate: 1.0000e-03\n",
      "Epoch 62/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0049 - learning_rate: 1.0000e-03\n",
      "Epoch 63/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0048 - learning_rate: 1.0000e-03\n",
      "Epoch 64/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0047 - learning_rate: 1.0000e-03\n",
      "Epoch 65/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0046 - learning_rate: 1.0000e-03\n",
      "Epoch 66/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0046 - learning_rate: 1.0000e-03\n",
      "Epoch 67/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0045 - learning_rate: 1.0000e-03\n",
      "Epoch 68/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0044 - learning_rate: 1.0000e-03\n",
      "Epoch 69/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0044 - learning_rate: 1.0000e-03\n",
      "Epoch 70/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0043 - learning_rate: 1.0000e-03\n",
      "Epoch 71/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0042 - learning_rate: 1.0000e-03\n",
      "Epoch 72/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0042 - learning_rate: 1.0000e-03\n",
      "Epoch 73/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0041 - learning_rate: 1.0000e-03\n",
      "Epoch 74/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0040 - learning_rate: 1.0000e-03\n",
      "Epoch 75/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0040 - learning_rate: 1.0000e-03\n",
      "Epoch 76/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0039 - learning_rate: 1.0000e-03\n",
      "Epoch 77/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0039 - learning_rate: 1.0000e-03\n",
      "Epoch 78/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0038 - learning_rate: 1.0000e-03\n",
      "Epoch 79/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0038 - learning_rate: 1.0000e-03\n",
      "Epoch 80/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0037 - learning_rate: 1.0000e-03\n",
      "Epoch 81/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0037 - learning_rate: 1.0000e-03\n",
      "Epoch 82/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0036 - learning_rate: 1.0000e-03\n",
      "Epoch 83/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0036 - learning_rate: 1.0000e-03\n",
      "Epoch 84/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0035 - learning_rate: 1.0000e-03\n",
      "Epoch 85/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0035 - learning_rate: 1.0000e-03\n",
      "Epoch 86/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0034 - learning_rate: 1.0000e-03\n",
      "Epoch 87/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0034 - learning_rate: 1.0000e-03\n",
      "Epoch 88/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0033 - learning_rate: 1.0000e-03\n",
      "Epoch 89/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0033 - learning_rate: 1.0000e-03\n",
      "Epoch 90/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0032 - learning_rate: 1.0000e-03\n",
      "Epoch 91/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0032 - learning_rate: 1.0000e-03\n",
      "Epoch 92/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0031 - learning_rate: 1.0000e-03\n",
      "Epoch 93/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0031 - learning_rate: 1.0000e-03\n",
      "Epoch 94/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0030 - learning_rate: 1.0000e-03\n",
      "Epoch 95/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0030 - learning_rate: 1.0000e-03\n",
      "Epoch 96/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0030 - learning_rate: 1.0000e-03\n",
      "Epoch 97/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0029 - learning_rate: 1.0000e-03\n",
      "Epoch 98/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0029 - learning_rate: 1.0000e-03\n",
      "Epoch 99/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0028 - learning_rate: 1.0000e-03\n",
      "Epoch 100/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0028 - learning_rate: 1.0000e-03\n",
      "Epoch 101/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0027 - learning_rate: 1.0000e-03\n",
      "Epoch 102/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0027 - learning_rate: 1.0000e-03\n",
      "Epoch 103/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0027 - learning_rate: 1.0000e-03\n",
      "Epoch 104/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0026 - learning_rate: 1.0000e-03\n",
      "Epoch 105/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0026 - learning_rate: 1.0000e-03\n",
      "Epoch 106/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0026 - learning_rate: 1.0000e-03\n",
      "Epoch 107/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0025 - learning_rate: 1.0000e-03\n",
      "Epoch 108/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0025 - learning_rate: 1.0000e-03\n",
      "Epoch 109/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0025 - learning_rate: 1.0000e-03\n",
      "Epoch 110/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0024 - learning_rate: 1.0000e-03\n",
      "Epoch 111/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0024 - learning_rate: 1.0000e-03\n",
      "Epoch 112/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0023 - learning_rate: 1.0000e-03\n",
      "Epoch 113/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0023 - learning_rate: 1.0000e-03\n",
      "Epoch 114/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0023 - learning_rate: 1.0000e-03\n",
      "Epoch 115/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0023 - learning_rate: 1.0000e-03\n",
      "Epoch 116/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0022 - learning_rate: 1.0000e-03\n",
      "Epoch 117/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0022 - learning_rate: 1.0000e-03\n",
      "Epoch 118/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0022 - learning_rate: 1.0000e-03\n",
      "Epoch 119/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0021 - learning_rate: 1.0000e-03\n",
      "Epoch 120/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0021 - learning_rate: 1.0000e-03\n",
      "Epoch 121/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-03\n",
      "Epoch 122/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0020 - learning_rate: 1.0000e-03\n",
      "Epoch 123/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-03\n",
      "Epoch 124/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-03\n",
      "Epoch 125/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-03\n",
      "Epoch 126/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-03\n",
      "Epoch 127/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-03\n",
      "Epoch 128/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0018 - learning_rate: 1.0000e-03\n",
      "Epoch 129/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0018 - learning_rate: 1.0000e-03\n",
      "Epoch 130/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0018 - learning_rate: 1.0000e-03\n",
      "Epoch 131/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0017 - learning_rate: 1.0000e-03\n",
      "Epoch 132/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0017 - learning_rate: 1.0000e-03\n",
      "Epoch 133/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0017 - learning_rate: 1.0000e-03\n",
      "Epoch 134/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0016 - learning_rate: 1.0000e-03\n",
      "Epoch 135/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0016 - learning_rate: 1.0000e-03\n",
      "Epoch 136/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0016 - learning_rate: 1.0000e-03\n",
      "Epoch 137/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0016 - learning_rate: 1.0000e-03\n",
      "Epoch 138/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0016 - learning_rate: 1.0000e-03\n",
      "Epoch 139/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 140/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 141/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 142/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 143/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 144/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0014 - learning_rate: 1.0000e-03\n",
      "Epoch 145/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 146/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0015 - learning_rate: 1.0000e-03\n",
      "Epoch 147/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-03\n",
      "Epoch 148/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0022 - learning_rate: 1.0000e-03\n",
      "Epoch 149/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0026 - learning_rate: 1.0000e-03\n",
      "Epoch 150/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0029 - learning_rate: 1.0000e-03\n",
      "Epoch 151/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-03\n",
      "Epoch 152/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0224 - learning_rate: 1.0000e-03\n",
      "Epoch 153/300\n",
      "\u001B[1m29/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0304 \n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0324 - learning_rate: 1.0000e-03\n",
      "Epoch 154/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0094 - learning_rate: 1.0000e-04\n",
      "Epoch 155/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0066 - learning_rate: 1.0000e-04\n",
      "Epoch 156/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0056 - learning_rate: 1.0000e-04\n",
      "Epoch 157/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 158/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0042 - learning_rate: 1.0000e-04\n",
      "Epoch 159/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0039 - learning_rate: 1.0000e-04\n",
      "Epoch 160/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0036 - learning_rate: 1.0000e-04\n",
      "Epoch 161/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0034 - learning_rate: 1.0000e-04\n",
      "Epoch 162/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0032 - learning_rate: 1.0000e-04\n",
      "Epoch 163/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0030 - learning_rate: 1.0000e-04\n",
      "Epoch 164/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0029 - learning_rate: 1.0000e-04\n",
      "Epoch 165/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0028 - learning_rate: 1.0000e-04\n",
      "Epoch 166/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0027 - learning_rate: 1.0000e-04\n",
      "Epoch 167/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 168/300\n",
      "\u001B[1m27/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0024 \n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 169/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0022 - learning_rate: 1.0000e-05\n",
      "Epoch 170/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0021 - learning_rate: 1.0000e-05\n",
      "Epoch 171/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0021 - learning_rate: 1.0000e-05\n",
      "Epoch 172/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0021 - learning_rate: 1.0000e-05\n",
      "Epoch 173/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0021 - learning_rate: 1.0000e-05\n",
      "Epoch 174/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0021 - learning_rate: 1.0000e-05\n",
      "Epoch 175/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 176/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 177/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 178/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 179/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 180/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 181/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 182/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 183/300\n",
      "\u001B[1m31/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0020 \n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 184/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0020 - learning_rate: 1.0000e-06\n",
      "Epoch 185/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0020 - learning_rate: 1.0000e-06\n",
      "Epoch 186/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 187/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 188/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 189/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 190/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 191/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 192/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 193/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 194/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 195/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 196/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 197/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 198/300\n",
      "\u001B[1m33/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-06\n",
      "Epoch 199/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 200/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 201/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 202/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 203/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 204/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 205/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 206/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 207/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 208/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 209/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 210/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 211/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 212/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 213/300\n",
      "\u001B[1m36/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-07\n",
      "Epoch 214/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 215/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 216/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 217/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 218/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 219/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 220/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 221/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 222/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 223/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 224/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 225/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 226/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 227/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 228/300\n",
      "\u001B[1m35/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0019\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0019 - learning_rate: 1.0000e-08\n",
      "Epoch 229/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 230/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 231/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 232/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 233/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 234/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 235/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 236/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 237/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 238/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 239/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 240/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 241/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 242/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 243/300\n",
      "\u001B[1m28/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019\n",
      "Epoch 243: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-09\n",
      "Epoch 244/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 245/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 246/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 247/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 248/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 249/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 250/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 251/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 252/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 253/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 254/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 255/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 256/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 257/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 258/300\n",
      "\u001B[1m35/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019\n",
      "Epoch 258: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-10\n",
      "Epoch 259/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 260/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 261/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 262/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 263/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 264/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 265/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 266/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 267/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 268/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 269/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 270/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 271/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 272/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 273/300\n",
      "\u001B[1m33/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019\n",
      "Epoch 273: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-11\n",
      "Epoch 274/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 275/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 276/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 277/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 278/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 279/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 280/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 281/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 282/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 283/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 284/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 285/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 286/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 287/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 288/300\n",
      "\u001B[1m30/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019\n",
      "Epoch 288: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0019 - learning_rate: 1.0000e-12\n",
      "Epoch 289/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 290/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 291/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 292/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 293/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 294/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 295/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 296/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 297/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 298/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 299/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Epoch 300/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0019 - learning_rate: 1.0000e-13\n",
      "Done Training: model2\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step \n",
      "[[134912.62]\n",
      " [174520.33]\n",
      " [ 86001.38]\n",
      " [160157.72]\n",
      " [132168.88]]\n",
      "Training Model: model3\n",
      "Epoch 1/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 5ms/step - loss: 64.5244 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 4.2152 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 1.6756 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 1.1899 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.8989 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.7223 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.5932 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4851 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4010 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.3349 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2840 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.2453 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2116 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1852 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1595 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1545 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1334 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1360 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1327 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.2186 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1262 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1366 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1106 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1747 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1161 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0765 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0765 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0649 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0637 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0638 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0598 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0620 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0556 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0567 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0459 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0494 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0455 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0445 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0457 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0436 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0428 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0400 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0388 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0388 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0365 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0398 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0520 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0823 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1561 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2764 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5378 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.3613 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1313 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.1136 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0739 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0927 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0829 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0953 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0749 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001B[1m17/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0619\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0602 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0558 - learning_rate: 1.0000e-03\n",
      "Epoch 62/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0282 - learning_rate: 1.0000e-03\n",
      "Epoch 63/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0254 - learning_rate: 1.0000e-03\n",
      "Epoch 64/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0229 - learning_rate: 1.0000e-03\n",
      "Epoch 65/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0214 - learning_rate: 1.0000e-03\n",
      "Epoch 66/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0203 - learning_rate: 1.0000e-03\n",
      "Epoch 67/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0196 - learning_rate: 1.0000e-03\n",
      "Epoch 68/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0190 - learning_rate: 1.0000e-03\n",
      "Epoch 69/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0186 - learning_rate: 1.0000e-03\n",
      "Epoch 70/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0183 - learning_rate: 1.0000e-03\n",
      "Epoch 71/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0180 - learning_rate: 1.0000e-03\n",
      "Epoch 72/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 73/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 74/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 75/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 76/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 77/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 78/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 79/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 80/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0165 - learning_rate: 1.0000e-03\n",
      "Epoch 81/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 82/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0162 - learning_rate: 1.0000e-03\n",
      "Epoch 83/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0161 - learning_rate: 1.0000e-03\n",
      "Epoch 84/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0160 - learning_rate: 1.0000e-03\n",
      "Epoch 85/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0159 - learning_rate: 1.0000e-03\n",
      "Epoch 86/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0158 - learning_rate: 1.0000e-03\n",
      "Epoch 87/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0157 - learning_rate: 1.0000e-03\n",
      "Epoch 88/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0156 - learning_rate: 1.0000e-03\n",
      "Epoch 89/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0155 - learning_rate: 1.0000e-03\n",
      "Epoch 90/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0157 - learning_rate: 1.0000e-03\n",
      "Epoch 91/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0153 - learning_rate: 1.0000e-03\n",
      "Epoch 92/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0152 - learning_rate: 1.0000e-03\n",
      "Epoch 93/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0151 - learning_rate: 1.0000e-03\n",
      "Epoch 94/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0150 - learning_rate: 1.0000e-03\n",
      "Epoch 95/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0149 - learning_rate: 1.0000e-03\n",
      "Epoch 96/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0148 - learning_rate: 1.0000e-03\n",
      "Epoch 97/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0148 - learning_rate: 1.0000e-03\n",
      "Epoch 98/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0147 - learning_rate: 1.0000e-03\n",
      "Epoch 99/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0146 - learning_rate: 1.0000e-03\n",
      "Epoch 100/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0145 - learning_rate: 1.0000e-03\n",
      "Epoch 101/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0144 - learning_rate: 1.0000e-03\n",
      "Epoch 102/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0143 - learning_rate: 1.0000e-03\n",
      "Epoch 103/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0143 - learning_rate: 1.0000e-03\n",
      "Epoch 104/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0142 - learning_rate: 1.0000e-03\n",
      "Epoch 105/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0142 - learning_rate: 1.0000e-03\n",
      "Epoch 106/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0140 - learning_rate: 1.0000e-03\n",
      "Epoch 107/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0140 - learning_rate: 1.0000e-03\n",
      "Epoch 108/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0139 - learning_rate: 1.0000e-03\n",
      "Epoch 109/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0138 - learning_rate: 1.0000e-03\n",
      "Epoch 110/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0138 - learning_rate: 1.0000e-03\n",
      "Epoch 111/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 22ms/step - loss: 0.0138 - learning_rate: 1.0000e-03\n",
      "Epoch 112/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - loss: 0.0137 - learning_rate: 1.0000e-03\n",
      "Epoch 113/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0136 - learning_rate: 1.0000e-03\n",
      "Epoch 114/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0136 - learning_rate: 1.0000e-03\n",
      "Epoch 115/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0135 - learning_rate: 1.0000e-03\n",
      "Epoch 116/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0135 - learning_rate: 1.0000e-03\n",
      "Epoch 117/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0134 - learning_rate: 1.0000e-03\n",
      "Epoch 118/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0134 - learning_rate: 1.0000e-03\n",
      "Epoch 119/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0133 - learning_rate: 1.0000e-03\n",
      "Epoch 120/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0133 - learning_rate: 1.0000e-03\n",
      "Epoch 121/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0132 - learning_rate: 1.0000e-03\n",
      "Epoch 122/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0132 - learning_rate: 1.0000e-03\n",
      "Epoch 123/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0131 - learning_rate: 1.0000e-03\n",
      "Epoch 124/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0132 - learning_rate: 1.0000e-03\n",
      "Epoch 125/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0131 - learning_rate: 1.0000e-03\n",
      "Epoch 126/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0131 - learning_rate: 1.0000e-03\n",
      "Epoch 127/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0130 - learning_rate: 1.0000e-03\n",
      "Epoch 128/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0130 - learning_rate: 1.0000e-03\n",
      "Epoch 129/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0129 - learning_rate: 1.0000e-03\n",
      "Epoch 130/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0129 - learning_rate: 1.0000e-03\n",
      "Epoch 131/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0129 - learning_rate: 1.0000e-03\n",
      "Epoch 132/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0129 - learning_rate: 1.0000e-03\n",
      "Epoch 133/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0128 - learning_rate: 1.0000e-03\n",
      "Epoch 134/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0127 - learning_rate: 1.0000e-03\n",
      "Epoch 135/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0128 - learning_rate: 1.0000e-03\n",
      "Epoch 136/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0127 - learning_rate: 1.0000e-03\n",
      "Epoch 137/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0127 - learning_rate: 1.0000e-03\n",
      "Epoch 138/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0125 - learning_rate: 1.0000e-03\n",
      "Epoch 139/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0126 - learning_rate: 1.0000e-03\n",
      "Epoch 140/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0125 - learning_rate: 1.0000e-03\n",
      "Epoch 141/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0125 - learning_rate: 1.0000e-03\n",
      "Epoch 142/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0125 - learning_rate: 1.0000e-03\n",
      "Epoch 143/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0124 - learning_rate: 1.0000e-03\n",
      "Epoch 144/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0123 - learning_rate: 1.0000e-03\n",
      "Epoch 145/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0124 - learning_rate: 1.0000e-03\n",
      "Epoch 146/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0123 - learning_rate: 1.0000e-03\n",
      "Epoch 147/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0123 - learning_rate: 1.0000e-03\n",
      "Epoch 148/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0122 - learning_rate: 1.0000e-03\n",
      "Epoch 149/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0122 - learning_rate: 1.0000e-03\n",
      "Epoch 150/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0121 - learning_rate: 1.0000e-03\n",
      "Epoch 151/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0121 - learning_rate: 1.0000e-03\n",
      "Epoch 152/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0120 - learning_rate: 1.0000e-03\n",
      "Epoch 153/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0120 - learning_rate: 1.0000e-03\n",
      "Epoch 154/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0120 - learning_rate: 1.0000e-03\n",
      "Epoch 155/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0119 - learning_rate: 1.0000e-03\n",
      "Epoch 156/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0119 - learning_rate: 1.0000e-03\n",
      "Epoch 157/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0118 - learning_rate: 1.0000e-03\n",
      "Epoch 158/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0118 - learning_rate: 1.0000e-03\n",
      "Epoch 159/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0117 - learning_rate: 1.0000e-03\n",
      "Epoch 160/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0117 - learning_rate: 1.0000e-03\n",
      "Epoch 161/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0116 - learning_rate: 1.0000e-03\n",
      "Epoch 162/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0116 - learning_rate: 1.0000e-03\n",
      "Epoch 163/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0116 - learning_rate: 1.0000e-03\n",
      "Epoch 164/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0115 - learning_rate: 1.0000e-03\n",
      "Epoch 165/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0115 - learning_rate: 1.0000e-03\n",
      "Epoch 166/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0115 - learning_rate: 1.0000e-03\n",
      "Epoch 167/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0114 - learning_rate: 1.0000e-03\n",
      "Epoch 168/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0114 - learning_rate: 1.0000e-03\n",
      "Epoch 169/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0114 - learning_rate: 1.0000e-03\n",
      "Epoch 170/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0113 - learning_rate: 1.0000e-03\n",
      "Epoch 171/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0113 - learning_rate: 1.0000e-03\n",
      "Epoch 172/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0112 - learning_rate: 1.0000e-03\n",
      "Epoch 173/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0112 - learning_rate: 1.0000e-03\n",
      "Epoch 174/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0112 - learning_rate: 1.0000e-03\n",
      "Epoch 175/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0112 - learning_rate: 1.0000e-03\n",
      "Epoch 176/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0111 - learning_rate: 1.0000e-03\n",
      "Epoch 177/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0111 - learning_rate: 1.0000e-03\n",
      "Epoch 178/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0111 - learning_rate: 1.0000e-03\n",
      "Epoch 179/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 180/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 181/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 182/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 183/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 184/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 185/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 186/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 187/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 188/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 189/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 190/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 191/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 192/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 193/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 194/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 195/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0107 - learning_rate: 1.0000e-03\n",
      "Epoch 196/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0107 - learning_rate: 1.0000e-03\n",
      "Epoch 197/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0107 - learning_rate: 1.0000e-03\n",
      "Epoch 198/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0107 - learning_rate: 1.0000e-03\n",
      "Epoch 199/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 200/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 201/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 202/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0109 - learning_rate: 1.0000e-03\n",
      "Epoch 203/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0108 - learning_rate: 1.0000e-03\n",
      "Epoch 204/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 205/300\n",
      "\u001B[1m23/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0109\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 206/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0126 - learning_rate: 1.0000e-04\n",
      "Epoch 207/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0089 - learning_rate: 1.0000e-04\n",
      "Epoch 208/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0087 - learning_rate: 1.0000e-04\n",
      "Epoch 209/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0086 - learning_rate: 1.0000e-04\n",
      "Epoch 210/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0086 - learning_rate: 1.0000e-04\n",
      "Epoch 211/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0085 - learning_rate: 1.0000e-04\n",
      "Epoch 212/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0085 - learning_rate: 1.0000e-04\n",
      "Epoch 213/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0084 - learning_rate: 1.0000e-04\n",
      "Epoch 214/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0083 - learning_rate: 1.0000e-04\n",
      "Epoch 215/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0083 - learning_rate: 1.0000e-04\n",
      "Epoch 216/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0083 - learning_rate: 1.0000e-04\n",
      "Epoch 217/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 218/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 219/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 220/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 221/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 222/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 223/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 224/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 225/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 226/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 227/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 228/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 229/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 230/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 231/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 232/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 233/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 234/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 235/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 236/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 237/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 238/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 239/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 240/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 241/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 242/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 243/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 244/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 245/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 246/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 247/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 248/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 249/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 250/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0078 - learning_rate: 1.0000e-04\n",
      "Epoch 251/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 252/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 253/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 254/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 255/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 256/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0077 - learning_rate: 1.0000e-04\n",
      "Epoch 257/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 258/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 259/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 260/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 261/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 262/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 263/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 264/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 265/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 266/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 267/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0076 - learning_rate: 1.0000e-04\n",
      "Epoch 268/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 269/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 270/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 271/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 272/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 273/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 274/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 275/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 276/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 277/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 278/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 279/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 280/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0075 - learning_rate: 1.0000e-04\n",
      "Epoch 281/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 282/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 283/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 284/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 285/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 286/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 287/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 288/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 289/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 290/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 291/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 292/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 293/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 294/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Epoch 295/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0073 - learning_rate: 1.0000e-04\n",
      "Epoch 296/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0072 - learning_rate: 1.0000e-04\n",
      "Epoch 297/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0073 - learning_rate: 1.0000e-04\n",
      "Epoch 298/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0073 - learning_rate: 1.0000e-04\n",
      "Epoch 299/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.0073 - learning_rate: 1.0000e-04\n",
      "Epoch 300/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0073 - learning_rate: 1.0000e-04\n",
      "Done Training: model3\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step \n",
      "[[147954.95 ]\n",
      " [160466.86 ]\n",
      " [ 85676.805]\n",
      " [181973.47 ]\n",
      " [156601.12 ]]\n",
      "Training Model: model4\n",
      "Epoch 1/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 6ms/step - loss: 385.5905 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 10.0875 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 2.5843 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.3778 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.8442 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.7030 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.5441 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.4026 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.2927 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.2066 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1559 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.1237 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.1105 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0896 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0825 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0707 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0629 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0590 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0584 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0570 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0536 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0456 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0447 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0421 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0477 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0530 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0464 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0475 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0490 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0499 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0560 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0881 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0839 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0861 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1451 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.2238 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.2804 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.3177 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.2427 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0393 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0208 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0162 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0158 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0154 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0150 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0148 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0146 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0144 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0143 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0141 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0139 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0138 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0131 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0133 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0125 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0123 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0123 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0117 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0115 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0114 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0112 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0109 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0106 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0103 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0102 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0096 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0096 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0096 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0091 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0089 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0085 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0084 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0079 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0084 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0084 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0082 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0085 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0080 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0081 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0085 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0096 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0156 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0248 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0578 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0734 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0445 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0551 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0417 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0333 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001B[1m34/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0361\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0371 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 97/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0131 - learning_rate: 1.0000e-03\n",
      "Epoch 98/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0117 - learning_rate: 1.0000e-03\n",
      "Epoch 99/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0110 - learning_rate: 1.0000e-03\n",
      "Epoch 100/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0105 - learning_rate: 1.0000e-03\n",
      "Epoch 101/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0102 - learning_rate: 1.0000e-03\n",
      "Epoch 102/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0099 - learning_rate: 1.0000e-03\n",
      "Epoch 103/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0096 - learning_rate: 1.0000e-03\n",
      "Epoch 104/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0095 - learning_rate: 1.0000e-03\n",
      "Epoch 105/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0093 - learning_rate: 1.0000e-03\n",
      "Epoch 106/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0092 - learning_rate: 1.0000e-03\n",
      "Epoch 107/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0090 - learning_rate: 1.0000e-03\n",
      "Epoch 108/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0089 - learning_rate: 1.0000e-03\n",
      "Epoch 109/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0088 - learning_rate: 1.0000e-03\n",
      "Epoch 110/300\n",
      "\u001B[1m33/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0087\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0087 - learning_rate: 1.0000e-03\n",
      "Epoch 111/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0085 - learning_rate: 1.0000e-04\n",
      "Epoch 112/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0083 - learning_rate: 1.0000e-04\n",
      "Epoch 113/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 114/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 115/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 116/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0082 - learning_rate: 1.0000e-04\n",
      "Epoch 117/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 118/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 119/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 120/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 121/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 122/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 123/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 124/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 125/300\n",
      "\u001B[1m32/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0081\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0080 - learning_rate: 1.0000e-04\n",
      "Epoch 126/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0080 - learning_rate: 1.0000e-05\n",
      "Epoch 127/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 128/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 129/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 130/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 131/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 132/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 133/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 134/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 135/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 136/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 137/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 138/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 139/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 140/300\n",
      "\u001B[1m33/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0077\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 141/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 142/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 143/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 144/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 145/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 146/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 147/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 148/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 149/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 150/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 151/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 152/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 153/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 154/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 155/300\n",
      "\u001B[1m32/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0077\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-06\n",
      "Epoch 156/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 157/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 158/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 159/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 160/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 161/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 162/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 163/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 164/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 165/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 166/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 167/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 168/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 169/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 170/300\n",
      "\u001B[1m34/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0077\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-07\n",
      "Epoch 171/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 172/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 173/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 174/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 175/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 176/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 177/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 12ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 178/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 179/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 180/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 181/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 182/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 183/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 184/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 185/300\n",
      "\u001B[1m34/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0077\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-08\n",
      "Epoch 186/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 187/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 188/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 189/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 190/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 191/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 192/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 193/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 194/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 195/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 196/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 197/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 198/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 199/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 200/300\n",
      "\u001B[1m31/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0077\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-09\n",
      "Epoch 201/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 202/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 203/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 204/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 205/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 206/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 207/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 208/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 209/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 210/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 211/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 212/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 213/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 214/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 215/300\n",
      "\u001B[1m29/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0077\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-10\n",
      "Epoch 216/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 217/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 218/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 219/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 220/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 221/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 222/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 223/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 224/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 225/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 226/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 227/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 228/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 229/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 230/300\n",
      "\u001B[1m35/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-11\n",
      "Epoch 231/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 232/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 233/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 234/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 235/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 236/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 237/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 238/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 239/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 240/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 241/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 242/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 243/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 244/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 245/300\n",
      "\u001B[1m31/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0077\n",
      "Epoch 245: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-12\n",
      "Epoch 246/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 247/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 248/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 249/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 250/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 251/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 252/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 253/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 254/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 255/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 256/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 257/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 258/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 259/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 260/300\n",
      "\u001B[1m35/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076\n",
      "Epoch 260: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-13\n",
      "Epoch 261/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 262/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 263/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 264/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 265/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 266/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 267/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 268/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 269/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 270/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 271/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 272/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 273/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 274/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 275/300\n",
      "\u001B[1m29/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0077 \n",
      "Epoch 275: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-14\n",
      "Epoch 276/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 277/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 278/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 279/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 280/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 281/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 282/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 283/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 284/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0075 - learning_rate: 1.0000e-15\n",
      "Epoch 285/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 286/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 287/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 288/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 289/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 290/300\n",
      "\u001B[1m29/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0077\n",
      "Epoch 290: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-15\n",
      "Epoch 291/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 292/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 293/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 294/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 295/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 296/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 297/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 298/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 299/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Epoch 300/300\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0076 - learning_rate: 1.0000e-16\n",
      "Done Training: model4\n",
      "\u001B[1m37/37\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step \n",
      "[[149407.03]\n",
      " [179573.7 ]\n",
      " [ 86925.08]\n",
      " [153623.02]\n",
      " [137583.39]]\n",
      "[145000 178000  85000 175000 127000]\n",
      "Done\n",
      "Results:\n",
      "model1: Training_mse: 59518475.55849282, RMSE: 7714.82180989897\n",
      "model1: CV_mse: 1662462193973.6729, RMSE: 1289365.0351912265\n",
      "model2: Training_mse: 35763104.18977095, RMSE: 5980.226098549364\n",
      "model2: CV_mse: 634050203.073747, RMSE: 25180.353513677026\n",
      "model3: Training_mse: 115726354.26312315, RMSE: 10757.618428961085\n",
      "model3: CV_mse: 14111679349677.086, RMSE: 3756551.5236286973\n",
      "model4: Training_mse: 114096262.87905349, RMSE: 10681.58522313301\n",
      "model4: CV_mse: 590119859.7102914, RMSE: 24292.382750777895\n"
     ]
    }
   ],
   "source": [
    "# Training the models\n",
    "train_mse = []\n",
    "cv_mse = []\n",
    "\n",
    "models = build_nn_model()\n",
    "for model in models:\n",
    "    scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=15, verbose=1)\n",
    "    # compiling the loss on model\n",
    "    model.compile(\n",
    "        loss= tf.keras.losses.MeanSquaredError(),\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    )\n",
    "    \n",
    "    print(f'Training Model: {model.name}')\n",
    "    model.fit(X_train_scaled, np.log(Y_train), epochs= 300, verbose= 1, callbacks=[scheduler])\n",
    "    \n",
    "    print(f'Done Training: {model.name}')\n",
    "    \n",
    "    # Calculating mses for training set\n",
    "    \n",
    "    yhat = model.predict(X_train_scaled)\n",
    "    yhat = np.exp(yhat)\n",
    "    error = mean_squared_error(Y_train, yhat) / 2\n",
    "    train_mse.append(error)\n",
    "    \n",
    "    #  Calculating mse for cross validation set\n",
    "    \n",
    "    yhat_cv = model.predict(X_cv_scaled)\n",
    "    yhat_cv = np.exp(yhat_cv)\n",
    "    error_cv = mean_squared_error(Y_cv, yhat_cv) / 2\n",
    "    cv_mse.append(error_cv)\n",
    "    print(yhat[:5])\n",
    "\n",
    "print(Y_train[:5])\n",
    "print('Done')\n",
    "print('Results:')\n",
    "for i in range(len(models)):\n",
    "    print(f'{models[i].name}: Training_mse: {train_mse[i]}, RMSE: {np.sqrt(train_mse[i])}')  \n",
    "    print(f'{models[i].name}: CV_mse: {cv_mse[i]}, RMSE: {np.sqrt(cv_mse[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53c533a3c84f772e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T17:58:58.354251Z",
     "start_time": "2024-10-31T17:58:58.344231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is: model4\n"
     ]
    }
   ],
   "source": [
    "# Selecting the best model\n",
    "best_model = np.argmin(cv_mse)\n",
    "print(f'Best Model is: {models[best_model].name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe13eea53e4941b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T18:04:31.177772Z",
     "start_time": "2024-10-31T18:04:31.094957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data before processing...\n",
      "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
      "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
      "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
      "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
      "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
      "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
      "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
      "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
      "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
      "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       0      6    2010        WD         Normal  \n",
      "1   12500      6    2010        WD         Normal  \n",
      "2       0      3    2010        WD         Normal  \n",
      "3       0      6    2010        WD         Normal  \n",
      "4       0      1    2010        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "Data after preprocessing:\n",
      "   LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  MasVnrArea  \\\n",
      "0         80.0    11622            5            6       1961         0.0   \n",
      "1         81.0    14267            6            6       1958       108.0   \n",
      "2         74.0    13830            5            5       1997         0.0   \n",
      "3         78.0     9978            6            6       1998        20.0   \n",
      "4         43.0     5005            8            5       1992         0.0   \n",
      "\n",
      "   BsmtFinSF1  BsmtUnfSF  TotalBsmtSF  1stFlrSF  ...  BsmtFullBath  \\\n",
      "0       468.0      270.0        882.0       896  ...           0.0   \n",
      "1       923.0      406.0       1329.0      1329  ...           0.0   \n",
      "2       791.0      137.0        928.0       928  ...           0.0   \n",
      "3       602.0      324.0        926.0       926  ...           0.0   \n",
      "4       263.0     1017.0       1280.0      1280  ...           0.0   \n",
      "\n",
      "   BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  KitchenAbvGr  TotRmsAbvGrd  \\\n",
      "0           0.0         1         0             2             1             5   \n",
      "1           0.0         1         1             3             1             6   \n",
      "2           0.0         2         1             3             1             6   \n",
      "3           0.0         2         1             3             1             7   \n",
      "4           0.0         2         0             2             1             5   \n",
      "\n",
      "   Fireplaces  GarageCars  PoolArea  \n",
      "0           0         1.0         0  \n",
      "1           0         1.0         0  \n",
      "2           1         2.0         0  \n",
      "3           1         2.0         0  \n",
      "4           0         2.0         0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Test input after scaling: [[ 0.44104726  0.08669258 -0.82044456  0.3722173  -0.32501586 -0.5978887\n",
      "   0.04789116 -0.67371792 -0.40811906 -0.70723826 -0.80192292 -1.20448634\n",
      "  -0.81869424 -0.24287002 -1.05556573 -0.76409752 -1.10677385 -0.21275711\n",
      "  -0.96456591 -0.95859215 -1.05654384 -0.07099284]\n",
      " [ 0.4855713   0.33263021 -0.08893368  0.3722173  -0.42285569  0.0275732\n",
      "   1.03944786 -0.36890371  0.60666536  0.41304409 -0.80192292 -0.37847856\n",
      "  -0.81869424 -0.24287002 -1.05556573  1.23694711  0.13621832 -0.21275711\n",
      "  -0.34690528 -0.95859215 -1.05654384 -0.07099284]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the test data\n",
    "test_data = './data/test.csv'\n",
    "df_test = modifying_data(test_data)\n",
    "df_test = df_test.values\n",
    "X_test_scaled = scaler.transform(df_test)\n",
    "print(f'Test input after scaling: {X_test_scaled[:2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4495d258708cc17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T18:05:15.507969Z",
     "start_time": "2024-10-31T18:05:15.253430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m46/46\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step\n",
      "[[122207.44]\n",
      " [155767.9 ]\n",
      " [176666.12]\n",
      " [195381.19]\n",
      " [180129.61]\n",
      " [179573.02]\n",
      " [194673.5 ]\n",
      " [160311.44]\n",
      " [211206.28]\n",
      " [124288.53]]\n"
     ]
    }
   ],
   "source": [
    "# Using the best model to predict the test data\n",
    "yhat_test = models[best_model].predict(X_test_scaled)\n",
    "yhat_test = np.exp(yhat_test)\n",
    "print(yhat_test[:10])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Selecting the neural network for the submission ",
   "id": "418ab51648b0bf8"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d47debdb9070718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459\n",
      "1459\n"
     ]
    }
   ],
   "source": [
    "# Selecting the neural network cause of its lover CV mse\n",
    "# Adding the prediction to the submission file\n",
    "df_sub = pd.read_csv('./data/sample_submission.csv')\n",
    "yhat_test = yhat_test.flatten()\n",
    "\n",
    "new_submission = pd.DataFrame({'Id': df_sub['Id'], 'SalePrice': yhat_test})\n",
    "new_submission.to_csv('./data/submission.csv', index=False)\n",
    "print(len(yhat_test))\n",
    "print(len(df_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae901b15-65ed-4661-8243-2684118e7402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
